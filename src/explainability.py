import joblib
import pandas as pd
import numpy as np  # è®°å¾—åŠ è¿™ä¸ªï¼
import shap
import matplotlib.pyplot as plt
from data_processing import load_and_split_data

# è®¾ç½® matplotlib é£æ ¼ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç ï¼ˆå¯é€‰ï¼Œæˆ–è€…ç”¨é»˜è®¤ï¼‰
plt.style.use('ggplot')

def explain_model(model_path='models/xgb_baseline.joblib', 
                  data_path='data/raw/UCI_Credit_Card.csv',
                  sample_size=500):
    """
    Generate SHAP explanations for the trained model.
    Args:
        sample_size: Number of samples to explain (smaller = faster)
    """
    print("="*60)
    print("ğŸ¤– MODEL EXPLAINABILITY ANALYSIS (SHAP)")
    print("="*60)
    
    # [1/6] Load data and model
    print("\n[1/6] Loading model and data...")
    X_train, X_test, y_train, y_test = load_and_split_data(data_path)
    pipeline = joblib.load(model_path)
    print(f"âœ“ Loaded {len(X_test)} test samples")
    
    # [2/6] Extract pipeline components
    print("\n[2/6] Extracting model components...")
    preprocessor = pipeline.named_steps['preprocessor']
    model = pipeline.named_steps['classifier']
    
    # [3/6] Preprocessing data for SHAP
    print("\n[3/6] Preprocessing data for SHAP...")
    X_test_transformed = preprocessor.transform(X_test)
    
    # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå·¥ç¨‹ä¼˜åŒ–ï¼‰
    if sample_size < len(X_test):
        print(f"   âš¡ Optimization: Using {sample_size} samples (randomly sampled) for speed")
        # éšæœºæŠ½æ ·æ¯”åªå–å‰Nä¸ªæ›´ç§‘å­¦
        indices = np.random.choice(X_test_transformed.shape[0], sample_size, replace=False)
        X_test_transformed = X_test_transformed[indices]
        y_test_subset = y_test.iloc[indices]
    else:
        y_test_subset = y_test
    
    # [4/6] Extracting feature names
    print("\n[4/6] Extracting feature names...")
    try:
        num_features = preprocessor.transformers_[0][2]
        cat_features = preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(preprocessor.transformers_[1][2])
        feature_names = list(num_features) + list(cat_features)
    except Exception as e:
        print(f"âš  Warning: Could not extract names, using indices. Error: {e}")
        feature_names = [f"Feature_{i}" for i in range(X_test_transformed.shape[1])]
    
    # [5/6] Calculating SHAP values
    print("\n[5/6] Calculating SHAP values...")
    print("   (This allows us to open the 'Black Box' of the model)")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test_transformed)
    
    # [6/6] Generating visualizations & Reports
    print("\n[6/6] Generating visualizations...")
    
    # --- å›¾è¡¨ 1: Summary Plot (å…¨å±€) ---
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)
    plt.savefig('shap_summary.png', bbox_inches='tight', dpi=300)
    plt.close()
    print("âœ“ Saved: shap_summary.png")
    
    # --- å›¾è¡¨ 2: Waterfall Plot (é«˜é£é™©ä¸ªæ¡ˆ) ---
    # æ‰¾ä¸€ä¸ªé¢„æµ‹æ¦‚ç‡æœ€é«˜çš„æ ·æœ¬ï¼ˆæœ€åƒåäººçš„æ ·æœ¬ï¼‰
    y_pred_proba = model.predict_proba(X_test_transformed)[:, 1]
    high_risk_idx = y_pred_proba.argmax()
    
    plt.figure(figsize=(10, 8))
    exp = shap.Explanation(
        values=shap_values[high_risk_idx], 
        base_values=explainer.expected_value, 
        data=X_test_transformed[high_risk_idx], 
        feature_names=feature_names
    )
    shap.waterfall_plot(exp, show=False)
    plt.title(f"Why did the model reject this client? (Prob: {y_pred_proba[high_risk_idx]:.1%})", fontsize=12)
    plt.savefig('shap_waterfall_high_risk.png', bbox_inches='tight', dpi=300)
    plt.close()
    print(f"âœ“ Saved: shap_waterfall_high_risk.png (Sample Index: {high_risk_idx})")
    
    # --- äº¤äº’å¼æŠ¥å‘Š (HTML) ---
    # æ³¨æ„ï¼šforce_plot ä¹Ÿæ˜¯ js æ¸²æŸ“ï¼Œä¿å­˜ä¸º html æœ€ç¨³å¦¥
    try:
        html_plot = shap.force_plot(explainer.expected_value, shap_values[:100], X_test_transformed[:100], feature_names=feature_names)
        shap.save_html('shap_interactive.html', html_plot)
        print("âœ“ Saved: shap_interactive.html")
    except Exception as e:
        print(f"âš  Skipped HTML generation: {e}")

    # --- æ”¹è¿›3: å…¬å¹³æ€§éšæ‚£æ£€æŸ¥ ---
    print("\n" + "-"*30)
    print("ğŸ•µï¸ FAIRNESS CHECK (SHAP Based)")
    sex_features = [f for f in feature_names if 'SEX' in f] # æŸ¥æ‰¾æ€§åˆ«ç›¸å…³ç‰¹å¾
    
    if sex_features:
        # è®¡ç®—å…¨å±€å¹³å‡ç»å¯¹ SHAP å€¼
        global_importances = np.abs(shap_values).mean(axis=0)
        total_importance = global_importances.sum()
        
        # è®¡ç®—æ€§åˆ«ç‰¹å¾çš„è´¡çŒ®åº¦
        sex_importance = sum([global_importances[feature_names.index(f)] for f in sex_features])
        sex_ratio = (sex_importance / total_importance) * 100
        
        print(f"   Gender Feature Importance: {sex_ratio:.2f}%")
        if sex_ratio > 5:
            print("   âš ï¸  ALERT: Model relies heavily on Gender (>5%)!")
        else:
            print("   âœ… PASS: Model relies minimaly on Gender (<5%).")
    else:
        print("   âš  'SEX' feature not found in feature names.")
    print("-"*30)

    print("\nDONE! All explanation assets are ready.")

if __name__ == "__main__":
    explain_model(sample_size=500)